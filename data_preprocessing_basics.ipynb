{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "106b2af2",
   "metadata": {},
   "source": [
    "# **Data Preprocessing: Scaling, Encoding, and Dimensionality Reduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe7e9a9",
   "metadata": {},
   "source": [
    "Goal= \"\"\"\n",
    "in this notebook i will get raw data ready for machine learning. \n",
    "we'll walks through some of the most common preprocessing steps you’ll need before training a model like scaling features, turning text into numbers, filling in missing values, & reducing the complexity of your data (PCA & t-SNE)   \n",
    "Each section is done both manually & with Scikit-Learn so I can really understand what’s happening under the hood while also learning how to use\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de36beac",
   "metadata": {},
   "source": [
    "## Scaling Data with StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a81051",
   "metadata": {},
   "source": [
    "### Manual Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e91ff862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "[-1.5666989  -1.21854359 -0.87038828 -0.52223297 -0.17407766  0.17407766\n",
      "  0.52223297  0.87038828  1.21854359  1.5666989 ]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X=[1,2,3,4,5,6,7,8,9,10]\n",
    "X=np.array(X)-np.average(X)\n",
    "print(np.average(X))\n",
    "X=X/np.sqrt(np.var(X))\n",
    "print(X)\n",
    "print(np.var(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c11830",
   "metadata": {},
   "source": [
    "### Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ef77598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n",
      "[[-1.5666989 ]\n",
      " [-1.21854359]\n",
      " [-0.87038828]\n",
      " [-0.52223297]\n",
      " [-0.17407766]\n",
      " [ 0.17407766]\n",
      " [ 0.52223297]\n",
      " [ 0.87038828]\n",
      " [ 1.21854359]\n",
      " [ 1.5666989 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import scale, MinMaxScaler, StandardScaler, RobustScaler\n",
    "X=np.array([[1],[2],[3],[4],[5],[6],[7],[8],[9],[10]])\n",
    "print(X.shape)\n",
    "scaler = StandardScaler()\n",
    "scaledX=scaler.fit_transform(X) #scaling & reduction use fit_transform so thats why we assign it to s.th else\n",
    "print(scaledX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e9e44d",
   "metadata": {},
   "source": [
    "## Encoding Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e735c2",
   "metadata": {},
   "source": [
    "### manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb67ae32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['2']\n",
      " ['3']\n",
      " ['4']\n",
      " ['1']\n",
      " ['2']]\n"
     ]
    }
   ],
   "source": [
    "data=np.array([['b'],['c'],['d'],['a'],['b']])\n",
    "encode=[1,2,3,4]\n",
    "meow=[]\n",
    "for i in range(len(data)):\n",
    "    if data[i][0]=='a':\n",
    "        data[i][0]=encode[0]\n",
    "\n",
    "    if data[i][0]=='b':\n",
    "        data[i][0]=encode[1]\n",
    "\n",
    "    if data[i][0]=='c':\n",
    "        data[i][0]=encode[2]\n",
    "\n",
    "    if data[i][0]=='d':\n",
    "        data[i][0]=encode[3]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1548a9de",
   "metadata": {},
   "source": [
    "### Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a870a98",
   "metadata": {},
   "source": [
    "#### Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "660c4d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [2.]\n",
      " [3.]\n",
      " [9.]\n",
      " [4.]\n",
      " [5.]\n",
      " [6.]\n",
      " [7.]\n",
      " [8.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "data=np.array([[1],[2],[3],['a'],[5],[6],[7],[8],[9],[10]])\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "encoding=OrdinalEncoder()\n",
    "encoded_data = encoding.fit_transform(data)\n",
    "print(encoded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d83ca",
   "metadata": {},
   "source": [
    "#### one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e7f419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "data=np.array([[1],[2],[3],['a'],[5],[6],[7],[8],[9],[10]])\n",
    "\n",
    "encoder = OneHotEncoder()  # set sparse=True to get sparse matrix\n",
    "Xencoded = encoder.fit_transform(data).toarray() # X should be 2D\n",
    "print(Xencoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db70aea8",
   "metadata": {},
   "source": [
    "## Handling Missing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bf741e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.   2.  ]\n",
      " [2.   3.  ]\n",
      " [3.   4.25]\n",
      " [4.   5.  ]\n",
      " [6.   7.  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "data=np.array([[1,2],[2,3],[3,np.nan],[4,5],[6,7]])\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "khiar=imp.fit_transform(data)\n",
    "#other methods:\n",
    "#* mean\n",
    "#* median\n",
    "#* most_frequent\n",
    "#* out of range => Turn into outliers\n",
    "print(khiar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d144c86a",
   "metadata": {},
   "source": [
    "## Feature Selection with Variance Threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e1b370e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reduced X has a shape of (5, 1)\n",
      "[[ 2]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [60]\n",
      " [ 7]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "data=np.array([[1,2],[2,3],[3,4],[4,60],[6,7]])\n",
    "VT = VarianceThreshold(threshold=10)\n",
    "khiar = VT.fit_transform(data)\n",
    "\n",
    "print(\"The reduced X has a shape of {}\".format(khiar.shape))\n",
    "print(khiar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceb8ff6",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction using PCA (Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37cee6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.11126984]\n",
      " [-1.69705627]\n",
      " [-0.28284271]\n",
      " [ 1.13137085]\n",
      " [ 3.95979797]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "data=np.array([[1,2],[2,3],[3,4],[4,5],[6,7]])\n",
    "pca = PCA(n_components=1)  # Choose number of components\n",
    "khiar = pca.fit_transform(data)\n",
    "print(khiar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c4328a",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction using t-SNE (Non-Linear)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "154c2282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 780.7292 ]\n",
      " [-607.5442 ]\n",
      " [-305.00314]\n",
      " [ -33.44689]\n",
      " [ 278.30508]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "data=np.array([[1,2],[2,3],[3,4],[4,5],[6,7]])\n",
    "\n",
    "tsne = TSNE(n_components=1, perplexity=2) # how many feature you want to keep   # \n",
    "X_tsne = tsne.fit_transform(data)\n",
    "print(X_tsne)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
